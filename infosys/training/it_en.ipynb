{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D-17UJnKmDL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sentencepiece -q\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/archive/en-it_train.csv'  # Update path as needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract 'en' and 'fr' translations from the 'translation' column\n",
        "df['en'] = df['translation'].apply(lambda x: eval(x)['en'])\n",
        "df['it'] = df['translation'].apply(lambda x: eval(x)['fr'])\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)\n",
        "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save data to CSVs for Hugging Face datasets\n",
        "train_data.to_csv('train.csv', index=False)\n",
        "val_data.to_csv('val.csv', index=False)\n",
        "test_data.to_csv('test.csv', index=False)\n"
      ],
      "metadata": {
        "id": "WeOksijYKrpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the data into Hugging Face datasets\n",
        "data_files = {\n",
        "    \"train\": \"train.csv\",\n",
        "    \"validation\": \"val.csv\",\n",
        "    \"test\": \"test.csv\"\n",
        "}\n",
        "dataset = load_dataset('csv', data_files=data_files)"
      ],
      "metadata": {
        "id": "wJl7OxlHKtRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Use a pretrained English-French translation model\n",
        "model_name = \"Helsinki-NLP/opus-mt-it-en\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['it']\n",
        "    targets = examples['en']\n",
        "\n",
        "    # Tokenize inputs and targets with padding and truncation\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "AvTTxy4TKvYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load the pretrained translation model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "STtZvxD6KxUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "# Define a data collator to handle padding during batching\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "KEGwGOx7Ky_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import numpy as np\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True\n",
        ")\n",
        "# Define the trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "6xGHFymoK2Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
        "print(\"Test Results:\", results)"
      ],
      "metadata": {
        "id": "hhRhnY_GK4LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and tokenizer to your Google Drive\n",
        "model.save_pretrained(\"/content/drive/MyDrive/models/it-en-translation_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/models/it-en-translation_model\")"
      ],
      "metadata": {
        "id": "KUtfinR8K52Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load the model and tokenizer from your saved path\n",
        "model_path = \"/content/drive/MyDrive/models/it-en-translation_model\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "rZDMVtcZK7XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence, model, tokenizer, max_length=50):\n",
        "    # Tokenize the input sentence\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    # Generate translation using the model\n",
        "    outputs = model.generate(**inputs, max_length=max_length, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the output tokens to text\n",
        "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return translation\n"
      ],
      "metadata": {
        "id": "YBz9keqAK9GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences to test\n",
        "test_sentences = [\n",
        "]\n",
        "\n",
        "# Generate translations\n",
        "for sentence in test_sentences:\n",
        "    english_translation = translate(sentence, model, tokenizer)\n",
        "    print(f\"italian: {sentence}\")\n",
        "    print(f\"English: {english_translation}\\n\")\n"
      ],
      "metadata": {
        "id": "kERKYuNAK-q_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}