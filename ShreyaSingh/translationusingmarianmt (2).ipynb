{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10120852,"sourceType":"datasetVersion","datasetId":6245021},{"sourceId":10122904,"sourceType":"datasetVersion","datasetId":6246540}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer\nimport torch\n\n# Define the path to the local directory containing the pretrained model files\nlocal_model_path = \"/kaggle/input/pretrainedd\"\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the tokenizer using the SentencePiece files directly\nprint(\"Loading tokenizer...\")\ntokenizer = MarianTokenizer.from_pretrained(\n    local_model_path, sp_model_kwargs={\"model_file\": f\"{local_model_path}/source.spm\"}\n)\n\n# Load the MarianMT model\nprint(\"Loading model...\")\nmodel = MarianMTModel.from_pretrained(local_model_path).to(device)\n\n# Example texts to translate\ntexts = [\n    \"Hello, how are you?\",\n    \"The weather is beautiful today.\",\n    \"I love programming and learning new things.\"\n]\n\n# Function to translate text\ndef translate_texts(texts, tokenizer, model, device):\n    translations = []\n    for text in texts:\n        # Tokenize and prepare input tensors\n        inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n        # Generate translation\n        translated = model.generate(**inputs)\n        # Decode and add to results\n        translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n        translations.append(translated_text)\n    return translations\n\n# Perform translations\nprint(\"Translating...\")\ntranslated_texts = translate_texts(texts, tokenizer, model, device)\n\n# Output results\nfor i, (src, tgt) in enumerate(zip(texts, translated_texts)):\n    print(f\"{i + 1}. Original: {src}\")\n    print(f\"   Translated: {tgt}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:27:21.769452Z","iopub.execute_input":"2024-12-26T11:27:21.769681Z","iopub.status.idle":"2024-12-26T11:27:44.221517Z","shell.execute_reply.started":"2024-12-26T11:27:21.769655Z","shell.execute_reply":"2024-12-26T11:27:44.220578Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading tokenizer...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"name":"stdout","text":"Loading model...\nTranslating...\n1. Original: Hello, how are you?\n   Translated: Bonjour, comment allez-vous ?\n2. Original: The weather is beautiful today.\n   Translated: Le temps est beau aujourd'hui.\n3. Original: I love programming and learning new things.\n   Translated: J'adore programmer et apprendre de nouvelles choses.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install sacrebleu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:27:44.224011Z","iopub.execute_input":"2024-12-26T11:27:44.224714Z","iopub.status.idle":"2024-12-26T11:27:52.426737Z","shell.execute_reply.started":"2024-12-26T11:27:44.224656Z","shell.execute_reply":"2024-12-26T11:27:52.425619Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.4.3)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (3.0.0)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport sacrebleu\n\n# Path to the folder containing your model files\nlocal_model_path = \"/kaggle/input/pretrainedd\"\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load tokenizer and model\nprint(\"Loading tokenizer and model...\")\ntokenizer = MarianTokenizer.from_pretrained(local_model_path)\nmodel = MarianMTModel.from_pretrained(local_model_path).to(device)\n\n# Example sentences (source) and their reference translations\nsource_sentences = [\n    \"Hello, how are you?\",\n    \"The weather is beautiful today.\",\n    \"I love programming and learning new things.\"\n]\n\nreference_translations = [\n    [\"Bonjour, comment ça va ?\"],\n    [\"Le temps est magnifique aujourd'hui.\"],\n    [\"J'adore programmer et apprendre de nouvelles choses.\"]\n]\n\n# Function to generate translations\ndef translate_sentences(sentences, tokenizer, model, device, num_beams=5):\n    translations = []\n    for sentence in sentences:\n        # Tokenize and prepare input tensors\n        inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n        # Generate translation with beam search\n        translated = model.generate(**inputs, num_beams=num_beams, no_repeat_ngram_size=2)\n        # Decode and append the result\n        translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n        translations.append(translated_text)\n    return translations\n\n# Generate translations for source sentences\nprint(\"Translating sentences...\")\ngenerated_translations = translate_sentences(source_sentences, tokenizer, model, device)\n\n# Compute BLEU score\nprint(\"Calculating BLEU score...\")\nbleu = sacrebleu.corpus_bleu(generated_translations, reference_translations)\n\n# Display the results\nfor i, (src, gen, ref) in enumerate(zip(source_sentences, generated_translations, reference_translations)):\n    print(f\"\\nSentence {i + 1}:\")\n    print(f\"Original: {src}\")\n    print(f\"Generated: {gen}\")\n    print(f\"Reference: {ref[0]}\")\n\nprint(f\"\\nBLEU score: {bleu.score}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:28:02.120856Z","iopub.execute_input":"2024-12-26T11:28:02.121571Z","iopub.status.idle":"2024-12-26T11:28:04.140953Z","shell.execute_reply.started":"2024-12-26T11:28:02.121531Z","shell.execute_reply":"2024-12-26T11:28:04.140028Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading tokenizer and model...\nTranslating sentences...\nCalculating BLEU score...\n\nSentence 1:\nOriginal: Hello, how are you?\nGenerated: Bonjour, comment allez-vous ?\nReference: Bonjour, comment ça va ?\n\nSentence 2:\nOriginal: The weather is beautiful today.\nGenerated: Le temps est beau aujourd'hui.\nReference: Le temps est magnifique aujourd'hui.\n\nSentence 3:\nOriginal: I love programming and learning new things.\nGenerated: J'adore programmer et apprendre de nouvelles choses.\nReference: J'adore programmer et apprendre de nouvelles choses.\n\nBLEU score: 34.98330125272253\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install sacremoses\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:00:11.185797Z","iopub.execute_input":"2024-12-07T06:00:11.186341Z","iopub.status.idle":"2024-12-07T06:00:20.317788Z","shell.execute_reply.started":"2024-12-07T06:00:11.186309Z","shell.execute_reply":"2024-12-07T06:00:20.316519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nfrom transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\nfrom datasets import Dataset\nimport pandas as pd\nimport torch\n\n# Load your pre-tokenized dataset\nfile_path = \"/kaggle/input/datasett/cleaned_en_fr_sample_tokenized (2).xlsx\"\ndata = pd.read_excel(file_path)\n\n# Define your tokenizer and model\nmodel_name = \"/kaggle/input/pretrainedd\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\n\n# Preprocess the data\ndef preprocess_data(row):\n    # Tokenize and pad sequences to max length (128)\n    inputs = tokenizer(row[\"source_tokens\"], truncation=True, padding=\"max_length\", max_length=128)\n    targets = tokenizer(row[\"target_tokens\"], truncation=True, padding=\"max_length\", max_length=128)\n\n    # Return the processed inputs and targets\n    return {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"],\n        \"labels\": targets[\"input_ids\"]\n    }\n\n# Convert pandas DataFrame to a Dataset object and apply preprocessing\ndataset = Dataset.from_pandas(data).map(preprocess_data, batched=True)\n\n# Split the dataset into train and test sets (90% for training, 10% for validation)\ntrain_test_split = dataset.train_test_split(test_size=0.1)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n# Data collator for handling padding during training\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# Set up training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_steps=500,\n    logging_dir=\"./logs\",\n    logging_steps=500,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    save_total_limit=2,\n    predict_with_generate=True,\n    fp16=torch.cuda.is_available(),\n)\n\n# Initialize the Seq2SeqTrainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n# Start training\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:02:58.189902Z","iopub.execute_input":"2024-12-07T06:02:58.190304Z","iopub.status.idle":"2024-12-07T06:22:37.395665Z","shell.execute_reply.started":"2024-12-07T06:02:58.190271Z","shell.execute_reply":"2024-12-07T06:22:37.394728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer\nimport sacrebleu\nimport torch\n\n# Load the pretrained model and tokenizer\nmodel_name = \"/kaggle/input/pretrainedd\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name).to(device)\n\n# Example source sentences and reference translations\nsource_sentences = [\n    \"She has been working at the company for over five years and has gained a lot of experience.\",\n    \"I will visit my grandparents this weekend if I finish all my work on time.\",\n    \"I love programming and learning new things.\"\n]\n\nreference_translations = [\n    [\"Elle travaille dans l'entreprise depuis plus de cinq ans et a acquis beaucoup d'expérience.\"],\n    [\"Je rendrai visite à mes grands-parents ce week-end si je termine tout mon travail à temps.\"],\n    [\"J'adore programmer et apprendre de nouvelles choses.\"]\n]\n\n# Function to translate sentences\ndef translate_sentences(sentences, tokenizer, model, device, num_beams=4):\n    translations = []\n    for sentence in sentences:\n        # Tokenize input sentence\n        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n        # Generate translation with beam search\n        outputs = model.generate(\n            **inputs, \n            num_beams=num_beams, \n            max_length=512, \n            no_repeat_ngram_size=2, \n            early_stopping=True\n        )\n        # Decode the generated translation\n        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        translations.append(translated_text)\n    return translations\n\n# Translate the source sentences\ngenerated_translations = translate_sentences(source_sentences, tokenizer, model, device)\n\n# Compute BLEU score\nbleu_score = sacrebleu.corpus_bleu(generated_translations, reference_translations)\n\n# Display results\nfor i, (src, gen, ref) in enumerate(zip(source_sentences, generated_translations, reference_translations)):\n    print(f\"\\nSentence {i + 1}:\")\n    print(f\"Original: {src}\")\n    print(f\"Generated: {gen}\")\n    print(f\"Reference: {ref[0]}\")\n\nprint(f\"\\nBLEU score: {bleu_score.score}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:39:54.747062Z","iopub.execute_input":"2024-12-07T06:39:54.747458Z","iopub.status.idle":"2024-12-07T06:39:57.025716Z","shell.execute_reply.started":"2024-12-07T06:39:54.747425Z","shell.execute_reply":"2024-12-07T06:39:57.024699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer\nimport torch\n\n# Load pretrained model and tokenizer\nmodel_name = \"/kaggle/input/pretrainedd\"  # Replace with your model path\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name).to(device)\n\n# Function to translate a single sentence\ndef translate_sentence(sentence, tokenizer, model, device, num_beams=4):\n    # Tokenize input sentence\n    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n    # Generate translation with beam search\n    outputs = model.generate(\n        **inputs, \n        num_beams=num_beams, \n        max_length=512, \n        no_repeat_ngram_size=2, \n        early_stopping=True\n    )\n    # Decode and return the generated translation\n    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return translated_text\n\n# Input your English sentence\nenglish_sentence = input(\"Enter an English sentence to translate: \")\n\n# Get the translation\ntranslated_french = translate_sentence(english_sentence, tokenizer, model, device)\n\n# Display the result\nprint(f\"\\nEnglish: {english_sentence}\")\nprint(f\"French: {translated_french}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:45:00.286361Z","iopub.execute_input":"2024-12-11T13:45:00.286820Z","iopub.status.idle":"2024-12-11T13:45:34.007938Z","shell.execute_reply.started":"2024-12-11T13:45:00.286734Z","shell.execute_reply":"2024-12-11T13:45:34.006614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:28:21.000684Z","iopub.execute_input":"2024-12-26T11:28:21.001643Z","iopub.status.idle":"2024-12-26T11:28:34.858088Z","shell.execute_reply.started":"2024-12-26T11:28:21.001608Z","shell.execute_reply":"2024-12-26T11:28:34.856946Z"}},"outputs":[{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.4.0)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.5.2 (from gradio)\n  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.0)\nRequirement already satisfied: huggingface-hub>=0.25.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.26.2)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\nRequirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.10.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.3.0)\nRequirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.10.1)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.2)\nCollecting ruff>=0.2.2 (from gradio)\n  Downloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.43.0-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.13.2)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.12.2)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.30.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.5.2->gradio) (2024.6.0)\nRequirement already satisfied: websockets<15.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.5.2->gradio) (12.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.0)\n  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.6.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (3.15.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->gradio) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.27.1)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (1.26.18)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: semantic-version, ruff, python-multipart, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\n  Attempting uninstall: python-multipart\n    Found existing installation: python-multipart 0.0.9\n    Uninstalling python-multipart-0.0.9:\n      Successfully uninstalled python-multipart-0.0.9\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.37.2\n    Uninstalling starlette-0.37.2:\n      Successfully uninstalled starlette-0.37.2\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.111.0\n    Uninstalling fastapi-0.111.0:\n      Successfully uninstalled fastapi-0.111.0\nSuccessfully installed fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 python-multipart-0.0.20 ruff-0.8.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import gradio as gr\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport nltk\n\nnltk.download(\"punkt\")  # For sentence tokenization\nfrom nltk.tokenize import sent_tokenize\n\n# Load pretrained model and tokenizer\nmodel_name = \"/kaggle/input/pretrainedd\"  # Replace with your model path\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name).to(device)\n\n# Function to translate a paragraph (from Code 3)\ndef translate_paragraph(paragraph):\n    # Split the paragraph into sentences\n    sentences = sent_tokenize(paragraph.strip())\n    translated_sentences = []\n\n    # Translate each sentence separately\n    for sentence in sentences:\n        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(device)\n        outputs = model.generate(\n            **inputs, \n            num_beams=4, \n            max_length=512, \n            no_repeat_ngram_size=2, \n            early_stopping=True\n        )\n        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        translated_sentences.append(translated_text)\n\n    # Join translated sentences back into a paragraph\n    return \" \".join(translated_sentences)\n\n# Function to translate the content of a file (from Code 2)\ndef translate_file(file):\n    # Read content from the uploaded file\n    with open(file.name, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        content = f.read()\n    \n    # Split content into sentences\n    sentences = sent_tokenize(content.strip())\n    translated_sentences = []\n\n    # Translate each sentence in chunks to avoid memory issues\n    batch_size = 10  # Process 10 sentences at a time\n    for i in range(0, len(sentences), batch_size):\n        batch = sentences[i:i+batch_size]\n        inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(device)\n        outputs = model.generate(\n            **inputs,\n            num_beams=4,\n            max_length=512,\n            no_repeat_ngram_size=2,\n            early_stopping=True\n        )\n        for output in outputs:\n            translated_text = tokenizer.decode(output, skip_special_tokens=True)\n            translated_sentences.append(translated_text)\n\n    # Combine the translated sentences into a single string\n    translated_content = \"\\n\".join(translated_sentences)\n\n    # Write the translated content to a new file\n    output_file = \"translated_file.txt\"\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(translated_content)\n\n    return output_file  # Return the file path for Gradio to offer as download\n\n# CSS styling for the Gradio interface (Black Background Theme)\ncustom_css = \"\"\"\n    .gradio-container {\n        background-color: #000000;\n        padding: 30px;\n        border-radius: 10px;\n        box-shadow: 0 6px 15px rgba(0, 0, 0, 0.5);\n        color: white;\n    }\n    .gradio-input, .gradio-output {\n        font-size: 18px;\n        padding: 15px;\n        border-radius: 5px;\n        margin-top: 10px;\n        background-color: #1e1e1e;\n        color: white;\n        border: 2px solid #444;\n    }\n    .gradio-input:focus, .gradio-output:focus {\n        outline: none;\n        border-color: #f39c12;\n    }\n    .gradio-button {\n        background-color: #f39c12;\n        color: white;\n        padding: 12px 20px;\n        border-radius: 5px;\n        font-size: 16px;\n        border: none;\n        cursor: pointer;\n    }\n    .gradio-button:hover {\n        background-color: #e67e22;\n    }\n    .gradio-title {\n        color: #f39c12;\n        font-size: 32px;\n        text-align: center;\n        font-family: 'Arial', sans-serif;\n        margin-bottom: 20px;\n    }\n    .gradio-description {\n        font-size: 18px;\n        color: #bdc3c7;\n        text-align: center;\n        font-family: 'Arial', sans-serif;\n        margin-bottom: 30px;\n    }\n    .gradio-footer {\n        font-size: 14px;\n        color: #7f8c8d;\n        text-align: center;\n    }\n\"\"\"\n\n# Create the interface with multiple tabs for File and Text translation\nwith gr.Blocks() as demo:\n    with gr.Tab(\"Text Translation\"):\n        # Text Translation\n        text_input = gr.Textbox(label=\"Enter Text\")\n        text_output = gr.Textbox(label=\"Translated Text\")\n        text_translate_button = gr.Button(\"Translate Text\")\n        text_translate_button.click(translate_paragraph, inputs=text_input, outputs=text_output)\n    \n    with gr.Tab(\"File Translation\"):\n        # File Translation\n        file_input = gr.File(label=\"Upload a text file\")\n        file_output = gr.File(label=\"Download Translated File\")\n        file_translate_button = gr.Button(\"Translate File\")\n        file_translate_button.click(translate_file, inputs=file_input, outputs=file_output)\n\ndemo.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:28:57.681387Z","iopub.execute_input":"2024-12-26T11:28:57.681733Z","iopub.status.idle":"2024-12-26T11:29:04.382087Z","shell.execute_reply.started":"2024-12-26T11:28:57.681703Z","shell.execute_reply":"2024-12-26T11:29:04.381260Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://dc32f387a85d02884d.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://dc32f387a85d02884d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}